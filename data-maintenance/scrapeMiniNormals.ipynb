{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Archived Mini Normals from Mafiascum.net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapy Structure/Lingo:\n",
    "**Spiders** extract data **items**, which Scrapy send one by one to a configured **item pipeline** (if there is possible) to do post-processing on the items.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant packages..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.item import Item, Field\n",
    "from scrapy.selector import Selector\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial variables..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "perpage = 25\n",
    "\n",
    "class PostItem(scrapy.Item):\n",
    "    pagelink = scrapy.Field()\n",
    "    forum = scrapy.Field()\n",
    "    thread = scrapy.Field()\n",
    "    number = scrapy.Field()\n",
    "    timestamp = scrapy.Field()\n",
    "    user = scrapy.Field()\n",
    "    content = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define what happens to scrape output..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# The following pipeline stores all scraped items (from all spiders) \n",
    "# into a single items.jl file, containing one item per line serialized \n",
    "# in JSON format:\n",
    "class JsonWriterPipeline(object):\n",
    "\n",
    "    # operations performed when spider starts\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('../data/posts.jsonl', 'w')\n",
    "\n",
    "    # when the spider finishes\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    # when the spider yields an item\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define spider..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class MafiaScumSpider(scrapy.Spider):\n",
    "    name = 'mafiascum'\n",
    "     \n",
    "    # settings\n",
    "    custom_settings = {'LOG_LEVEL': logging.WARNING,\n",
    "                      'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}}\n",
    "    \n",
    "    def start_requests(self):\n",
    "        \n",
    "        # define set of threads we're going to scrape from (ie all of them)\n",
    "        urls = [each[:each.find('\\n')] for each in open('../data/archive.txt').read().split('\\n\\n\\n')]\n",
    "        for url in tqdm(urls):\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    # get page counts and then do the REAL parse on every single page\n",
    "    def parse(self, response):\n",
    "        # find page count \n",
    "        try:\n",
    "            postcount = Selector(response).xpath(\n",
    "                '//div[@class=\"pagination\"]/text()').extract()\n",
    "            postcount = int(postcount[0][4:postcount[0].find(' ')])\n",
    "\n",
    "            # yield parse for every page of thread\n",
    "            for i in range(math.ceil(postcount/perpage)):\n",
    "                yield scrapy.Request(response.url+'&start='+str(i*perpage),\n",
    "                                    callback=self.parse_page)\n",
    "        except IndexError: # if can't, the thread probably doesn't exist\n",
    "            return\n",
    "        \n",
    "        \n",
    "    def parse_page(self, response):\n",
    "        # scan through posts on page and yield Post items for each\n",
    "        sel = Selector(response)\n",
    "        location = sel.xpath('//div[@id=\"page-body\"]/h2/a/@href').extract()[0]\n",
    "        forum = location[location.find('f=')+2:location.find('&t=')]\n",
    "        if location.count('&') == 1:\n",
    "            thread = location[location.find('&t=')+3:]\n",
    "        elif location.count('&') == 2:\n",
    "            thread = location[\n",
    "                location.find('&t=')+3:location.rfind('&')]\n",
    "        \n",
    "        posts = (sel.xpath('//div[@class=\"post bg1\"]') +\n",
    "                 sel.xpath('//div[@class=\"post bg2\"]'))\n",
    "        \n",
    "        for p in posts:\n",
    "            post = PostItem()\n",
    "            post['forum'] = forum\n",
    "            post['thread'] = thread\n",
    "            post['pagelink'] = response.url\n",
    "            try:\n",
    "                post['number'] = p.xpath(\n",
    "                    'div/div[@class=\"postbody\"]/p/a[2]/strong/text()').extract()[0][1:]\n",
    "            except IndexError:\n",
    "                post['number'] = p.xpath(\n",
    "                    'div[@class=\"postbody\"]/p/a[2]/strong/text()').extract()[0][1:]\n",
    "            \n",
    "            try:\n",
    "                post['timestamp'] = p.xpath(\n",
    "                    'div/div/p/text()[4]').extract()[0][23:-4]\n",
    "            except IndexError:\n",
    "                post['timestamp'] = p.xpath(\n",
    "                    'div[@class=\"postbody\"]/p/text()[4]').extract()[0][23:-4]\n",
    "            \n",
    "            try:\n",
    "                post['user'] = p.xpath('div/div/dl/dt/a/text()').extract()[0]\n",
    "            except IndexError:\n",
    "                post['user'] = '<<DELETED_USER>>'\n",
    "                \n",
    "            try:\n",
    "                post['content'] = p.xpath(\n",
    "                    'div/div/div[@class=\"content\"]').extract()[0][21:-6]\n",
    "            except IndexError:\n",
    "                post['content'] = p.xpath(\n",
    "                    'div[@class=\"postbody\"]/div[@class=\"content\"]').extract()[0][21:-6]\n",
    "            \n",
    "            yield post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start scraping..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "process.crawl(MafiaScumSpider)\n",
    "process.start()\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and output should be a json file in same directory as this notebook! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Results into Unique Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = open('../data/posts.jsonl')\n",
    "for post in posts:\n",
    "    with open('../data/posts/{}.jsonl'.format(json.loads(post)['thread']), 'a') as f:\n",
    "        f.write(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up Results\n",
    "Remove duplicate entries and sort by post number for every scraped game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through every file in directory\n",
    "for path, subdirs, files in os.walk('../data/posts'):\n",
    "    for name in files:\n",
    "        \n",
    "        # don't consider non-jsonl files\n",
    "        if name[-5:] != 'jsonl':\n",
    "            continue\n",
    "            \n",
    "        # load as dictionary, remove redundancies, and sort by post number\n",
    "        with open('../data/posts/{}'.format(name)) as f:\n",
    "            gameposts = [dict(t) for t in {tuple(d.items()) for d in [json.loads(l) for l in f]}]\n",
    "            gameposts = sorted(gameposts, key=lambda x: (int(x['thread']), int(x['number'])))\n",
    "        \n",
    "        # save result\n",
    "        with open('../data/posts/{}'.format(name), 'w') as f:\n",
    "            f.write('\\n'.join([json.dumps(post) for post in gameposts]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
